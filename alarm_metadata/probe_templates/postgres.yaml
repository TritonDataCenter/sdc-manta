#
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#

#
# Copyright (c) 2017, Joyent, Inc.
#

#
# amon probes for the "postgres" service
#
# For background information, see lib/alarms/index.js.  The format of this file
# is described in lib/alarms/metadata.js.
#

-
    event: upset.manta.postgres.db_filesystem_almost_full
    legacyName: database dataset space running low
    scope:
        service: postgres
    checks:
        -
            type: disk-usage
            config:
                path: "/manatee/pg"
                threshold: "20%"
                interval: 3600
    ka:
        title: Database filesystem almost full
        description: A database filesystem is running low on free space
        severity: critical
        response: No automated response will be taken.
        impact: >-
            There is no immediate impact, but if the filesystem fills up,
            service may become severely degraded.  End user requests may
            experience high error rates or increased latency.
        action: >-
            Identify the cause of excessive disk usage and resolve the
            underlying issue.

-
    event: upset.manta.postgres.sitter.log_error
    legacyName: manatee-logscan
    scope:
        service: postgres
    checks:
        -
            type: bunyan-log-scan
            config:
                smfServiceName: manatee-sitter
                fields:
                    level: FATAL
                threshold: 1
                period: 60
    ka:
        title: '"manatee-sitter" logged an error'
        description: The "manatee-sitter" service has logged an error.
        severity: major
        response: No automated response will be taken.
        impact: Unknown.
        action: >-
            Determine the scope of the problem based on the log message and
            resolve the underlying issue.

-
    event: upset.manta.postgres.snapshotter.log_error
    legacyName: manatee-snapshotter-logscan
    scope:
        service: postgres
    checks:
        -
            type: bunyan-log-scan
            config:
                smfServiceName: manatee-snapshotter
                fields:
                    level: FATAL
                threshold: 1
                period: 60
    ka:
        title: '"manatee-snapshotter" logged an error'
        description: The "manatee-snapshotter" service has logged an error.
        severity: major
        response: No automated response will be taken.
        impact: >-
            There is no immediate impact, but if snapshots are not being created
            regularly, then it may impossible to rebuild a Manatee peer and
            restore fault tolerance after a takeover event.  If snapshots are
            not being destroyed, disk space may become exhausted, resulting in
            significant service degradation.
        action: >-
            Determine the scope of the problem based on the log message and
            resolve the underlying issue.

-
    event: upset.manta.postgres.backupserver.log_error
    legacyName: manatee-backupserver-logscan
    scope:
        service: postgres
    checks:
        -
            type: bunyan-log-scan
            config:
                smfServiceName: manatee-backupserver
                fields:
                    level: FATAL
                threshold: 1
                period: 60
    ka:
        title: '"manatee-backupserver" logged an error'
        description: The "manatee-backupserver" service has logged an error.
        severity: major
        response: No automated response will be taken.
        impact: >-
            There is no immediate impact, but this service is required for
            Manatee rebuilds, which are necessary to restore fault tolerance
            after a takeover event.
        action: >-
            Determine the scope of the problem based on the log message and
            resolve the underlying issue.

-
    event: upset.manta.postgres.pg_dump.log_error
    legacyName: "pg_dump-logscan"
    scope:
        service: postgres
    checks:
        -
            type: log-scan
            config:
                path: "/var/log/manatee/pg_dump.log"
                match:
                    pattern: fatal
                threshold: 1
                period: 60
    ka:
        title: '"pg_dump" logged an error'
        description: The "pg_dump" service has logged an error.
        severity: major
        response: No automated response will be taken.
        impact: >-
            The daily backup for this metadata shard may be missing.  As a
            result, the garbage collection, auditing, and metering pipelines may
            not have completed.
        action: >-
            Determine the scope of the problem based on the log message and
            resolve the underlying issue.

-
    #
    # For the shard-wide probes (like the following one that faults when the
    # shard is unhealthy), it would be nice if this were one probe group per
    # shard, or at least one per zone, but that's not yet supported.
    #
    event: upset.manta.postgres.shard_unhealthy
    legacyName: manatee-stat
    scope:
        service: postgres
    checks:
        -
            type: cmd
            config:
                cmd: "/opt/smartdc/manatee/bin/manatee-stat-alarm.sh"
                interval: 60
                threshold: 5
                period: 360
                timeout: 60
                stdoutMatch:
                    pattern: fail
                    type: substring
    ka:
        title: Metadata shard is unhealthy
        description: A metadata shard is unhealthy
        severity: critical
        response: No automated response will be taken.
        impact: >-
            If the shard is completely offline, then end-user requests or job
            tasks operating on objects whose metadata is on the affected shard
            will fail.  If the shard is read-only, then new writes to
            directories on this shard will fail, but read operations and job
            tasks using objects on this shard will be unaffected.  If the shard
            is online for reads and writes, then only its fault tolerance is
            currently affected.
        action: >-
            Determine the scope of the problem and resolve the underying issue.
            See the "manatee-adm" command inside the PostgreSQL zones for
            details.

-
    #
    # Like the above template, it would be great if this were per-shard.
    #
    event: upset.manta.postgres.shard_transition
    legacyName: manatee-state-transition
    scope:
        service: postgres
    checks:
        -
            type: log-scan
            config:
                path: "/var/svc/log/manta-application-manatee-sitter:default.log"
                match:
                    pattern: finished transition
                threshold: 1
                period: 60
    ka:
        title: Metadata shard has experienced a takeover event
        description: >-
            A metadata shard has gone through a takeover transition.  This may
            be because the current synchronous peer took over for a primary that
            it believed had failed, or the current primary peer selected a new
            synchronous peer to replace one that had failed.  See "manatee-adm
            history" on this shard for details.
        severity: minor
        response: No automated response will be taken.
        impact: >-
            During the transition, some end user requests may have failed.  Some
            job tasks may have experienced higher dispatch latency or (in rarer
            cases) failures.  The transition has completed, and there should be
            no ongoing impact.

            If the primary peer has been deposed, then fault tolerance may be
            compromised until that peer has been rebuilt.
        action: >-
            There is likely no immediate action required, but it is recommended
            to verify the health of this shard using "manatee-adm".  You may
            need to rebuild a deposed peer.
