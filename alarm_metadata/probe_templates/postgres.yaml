#
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#

#
# Copyright 2019 Joyent, Inc.
#

#
# amon probes for the "postgres" service
#
# For background information, see lib/alarms/index.js.  The format of this file
# is described in lib/alarms/metadata.js.
#

-
    event: upset.manta.postgres.db_filesystem_almost_full
    legacyName: database dataset space running low
    scope:
        service: postgres
    checks:
        -
            type: disk-usage
            config:
                path: "/manatee/pg"
                threshold: "20%"
                interval: 3600
    ka:
        title: Database filesystem almost full
        description: A database filesystem is running low on free space
        severity: critical
        response: No automated response will be taken.
        impact: >-
            There is no immediate impact, but if the filesystem fills up,
            service may become severely degraded.  End user requests may
            experience high error rates or increased latency.
        action: >-
            Identify the cause of excessive disk usage and resolve the
            underlying issue.

-
    event: upset.manta.postgres.sitter.log_error
    legacyName: manatee-logscan
    scope:
        service: postgres
    checks:
        -
            type: bunyan-log-scan
            config:
                smfServiceName: manatee-sitter
                fields:
                    level: FATAL
                threshold: 1
                period: 60
    ka:
        title: '"manatee-sitter" logged an error'
        description: The "manatee-sitter" service has logged an error.
        severity: major
        response: No automated response will be taken.
        impact: Unknown.
        action: >-
            Determine the scope of the problem based on the log message and
            resolve the underlying issue.

-
    event: upset.manta.postgres.snapshotter.log_error
    legacyName: manatee-snapshotter-logscan
    scope:
        service: postgres
    checks:
        -
            type: bunyan-log-scan
            config:
                smfServiceName: manatee-snapshotter
                fields:
                    level: FATAL
                threshold: 1
                period: 60
    ka:
        title: '"manatee-snapshotter" logged an error'
        description: The "manatee-snapshotter" service has logged an error.
        severity: major
        response: No automated response will be taken.
        impact: >-
            There is no immediate impact, but if snapshots are not being created
            regularly, then it may impossible to rebuild a Manatee peer and
            restore fault tolerance after a takeover event.  If snapshots are
            not being destroyed, disk space may become exhausted, resulting in
            significant service degradation.
        action: >-
            Determine the scope of the problem based on the log message and
            resolve the underlying issue.

-
    event: upset.manta.postgres.backupserver.log_error
    legacyName: manatee-backupserver-logscan
    scope:
        service: postgres
    checks:
        -
            type: bunyan-log-scan
            config:
                smfServiceName: manatee-backupserver
                fields:
                    level: FATAL
                threshold: 1
                period: 60
    ka:
        title: '"manatee-backupserver" logged an error'
        description: The "manatee-backupserver" service has logged an error.
        severity: major
        response: No automated response will be taken.
        impact: >-
            There is no immediate impact, but this service is required for
            Manatee rebuilds, which are necessary to restore fault tolerance
            after a takeover event.
        action: >-
            Determine the scope of the problem based on the log message and
            resolve the underlying issue.

-
    event: upset.manta.postgres.pg_dump.log_error
    legacyName: "pg_dump-logscan"
    scope:
        service: postgres
    checks:
        -
            type: log-scan
            config:
                path: "/var/log/manatee/pgdump.log"
                match:
                    pattern: fatal
                threshold: 1
                period: 60
    ka:
        title: '"pg_dump" logged an error'
        description: The "pg_dump" service has logged an error.
        severity: major
        response: No automated response will be taken.
        impact: >-
            The daily backup for this metadata shard may be missing.  As a
            result, the garbage collection, auditing, and metering pipelines may
            not have completed.
        action: >-
            Determine the scope of the problem based on the log message and
            resolve the underlying issue.

-
    #
    # For the shard-wide probes (like the following one that faults when the
    # shard is unhealthy), it would be nice if this were one probe group per
    # shard, or at least one per zone, but that's not yet supported.
    #
    event: upset.manta.postgres.shard_unhealthy
    legacyName: manatee-stat
    scope:
        service: postgres
    checks:
        -
            type: cmd
            config:
                cmd: "/opt/smartdc/manatee/bin/manatee-stat-alarm.sh"
                interval: 60
                threshold: 5
                period: 360
                timeout: 60
                stdoutMatch:
                    pattern: fail
                    type: substring
    ka:
        title: Metadata shard is unhealthy
        description: A metadata shard is unhealthy
        severity: critical
        response: No automated response will be taken.
        impact: >-
            If the shard is completely offline, then end-user requests for
            objects whose metadata is on the affected shard will fail.  If the
            shard is read-only, then new writes to directories on this shard
            will fail, but read operations using objects on this shard will be
            unaffected.  If the shard is online for reads and writes, then only
            its fault tolerance is currently affected.
        action: >-
            Determine the scope of the problem and resolve the underying issue.
            See the "manatee-adm" command inside the PostgreSQL zones for
            details.

-
    #
    # Like the above template, it would be great if this were per-shard.
    #
    event: upset.manta.postgres.shard_transition
    legacyName: manatee-state-transition
    scope:
        service: postgres
    checks:
        -
            type: log-scan
            config:
                path: "/var/svc/log/manta-application-manatee-sitter:default.log"
                match:
                    pattern: finished transition
                threshold: 1
                period: 60
    ka:
        title: Metadata shard has experienced a takeover event
        description: >-
            A metadata shard has gone through a takeover transition.  This may
            be because the current synchronous peer took over for a primary that
            it believed had failed, or the current primary peer selected a new
            synchronous peer to replace one that had failed.  See "manatee-adm
            history" on this shard for details.
        severity: minor
        response: No automated response will be taken.
        impact: >-
            During the transition, some end user requests may have failed.  The
            transition has completed, and there should be no ongoing impact.

            If the primary peer has been deposed, then fault tolerance may be
            compromised until that peer has been rebuilt.
        action: >-
            There is likely no immediate action required, but it is recommended
            to verify the health of this shard using "manatee-adm".  You may
            need to rebuild a deposed peer.

-
    #
    # This query looks for non-autovacuum, long-running queries.  To reliably
    # identify error cases and cases where there are one or more long-running
    # query, the check fails if the output does NOT match the empty string.
    # There are some transient errors that can cause this to trip erroneously
    # while PostgreSQL is starting up, so we require two failures in 5 minutes
    # (the default value of "period").
    #
    event: upset.manta.postgres.long_query
    scope:
        service: postgres
    checks:
        -
            type: cmd
            config:
                cmd: "if [ $(psql -V | cut -f 3 -d ' ' | cut -f 1,2 -d '.') = '9.6' ]; then psql -U postgres -c \"SELECT pid,usename,client_addr,client_port,wait_event,wait_event_type,state,NOW() - query_start AS elapsed, query FROM pg_stat_activity WHERE state != 'idle' AND query NOT LIKE 'autovacuum: %' AND NOW() - query_start > interval '120 seconds'\" | tail +3; else psql -U postgres -c \"SELECT pid,usename,client_addr,client_port,waiting,state,NOW() - query_start AS elapsed, query FROM pg_stat_activity WHERE state != 'idle' AND query NOT LIKE 'autovacuum: %' AND NOW() - query_start > interval '120 seconds'\" | tail +3; fi"
                env:
                  PATH: "/opt/postgresql/current/bin:/opt/local/bin:$PATH"
                interval: 60
                timeout: 30
                threshold: 2
                stdoutMatch:
                    type: regex
                    invert: true
                    pattern: "\\(0 rows\\)"
    ka:
        title: Long-running query on metadata shard
        description: >-
            A database query has been running for several minutes.  Queries
            executed by Manta itself are designed to run quickly, so
            long-running queries are often indicative of an unhealthy shard.
            It is also possible that an administrator is running an expensive
            query.
        severity: major
        response: No automated response will be taken.
        impact: >-
            Besides being indicative of an unhealthy shard, long-running queries
            can impact the performance of other database queries.  They can also
            result in bloat in both the in-memory and on-disk database
            representations, resulting in significantly degraded performance of
            future queries.
        action: >-
            The alarm details include information about the long-running query.
            If the query is being run by an administrator, it should probably be
            cancelled.  Otherwise, assess the health of the shard and determine
            why the query is taking so long to run.
